use jsonrpsee::{
    core::{async_trait, RpcResult},
    proc_macros::rpc,
    types::error::{ErrorObjectOwned as JsonRpseeError, INTERNAL_ERROR_CODE, INTERNAL_ERROR_MSG},
};
use shc_common::types::{
    ChunkId, FileMetadata, HashT, StorageProofsMerkleTrieLayout, BCSV_KEY_TYPE, FILE_CHUNK_SIZE,
};
use sp_core::{sr25519::Pair as Sr25519Pair, Pair, H256};
use sp_keystore::{Keystore, KeystorePtr};
use sp_runtime::{AccountId32, Deserialize, KeyTypeId, Serialize};

use shc_file_manager::traits::{FileDataTrie, FileStorage, FileStorageError};
use shc_forest_manager::traits::ForestStorage;

use log::{debug, error};
use tokio::fs;

use std::{fmt::Debug, fs::File, io::Read, io::Write, path::PathBuf, sync::Arc};
use tokio::{fs::create_dir_all, sync::RwLock};

const LOG_TARGET: &str = "storage-hub-client-rpc";

pub struct StorageHubClientRpcConfig<FL, FS> {
    pub file_storage: Arc<RwLock<FL>>,
    pub forest_storage: Arc<RwLock<FS>>,
    pub keystore: KeystorePtr,
}

impl<FL, FS> Clone for StorageHubClientRpcConfig<FL, FS> {
    fn clone(&self) -> Self {
        Self {
            file_storage: self.file_storage.clone(),
            forest_storage: self.forest_storage.clone(),
            keystore: self.keystore.clone(),
        }
    }
}

impl<FL, FS> StorageHubClientRpcConfig<FL, FS>
where
    FL: FileStorage<StorageProofsMerkleTrieLayout> + Send + Sync,
    FS: ForestStorage<StorageProofsMerkleTrieLayout> + Send + Sync,
{
    pub fn new(
        file_storage: Arc<RwLock<FL>>,
        forest_storage: Arc<RwLock<FS>>,
        keystore: KeystorePtr,
    ) -> Self {
        Self {
            file_storage,
            forest_storage,
            keystore,
        }
    }
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct IncompleteFileStatus {
    pub file_metadata: FileMetadata,
    pub stored_chunks: u64,
    pub total_chunks: u64,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum SaveFileToDisk {
    FileNotFound,
    Success(FileMetadata),
    IncompleteFile(IncompleteFileStatus),
}

/// Provides an interface with the desired RPC method.
/// Used by the `rpc` macro from `jsonrpsee`
/// to generate the trait that is actually going to be implemented.
#[rpc(server, namespace = "storagehubclient")]
#[async_trait]
pub trait StorageHubClientApi {
    #[method(name = "loadFileInStorage")]
    async fn load_file_in_storage(
        &self,
        file_path: String,
        location: String,
        owner: AccountId32,
        bucket_id: H256,
    ) -> RpcResult<FileMetadata>;

    #[method(name = "saveFileToDisk")]
    async fn save_file_to_disk(
        &self,
        file_key: H256,
        file_path: String,
    ) -> RpcResult<SaveFileToDisk>;

    #[method(name = "getForestRoot")]
    async fn get_forest_root(&self) -> RpcResult<H256>;

    #[method(name = "insertBcsvKeys")]
    async fn insert_bcsv_keys(&self, seed: Option<String>) -> RpcResult<String>;

    #[method(name = "removeBcsvKeys")]
    async fn remove_bcsv_keys(&self, keystore_path: String) -> RpcResult<()>;
}

/// Stores the required objects to be used in our RPC method.
pub struct StorageHubClientRpc<FL, FS> {
    file_storage: Arc<RwLock<FL>>,
    forest_storage: Arc<RwLock<FS>>,
    keystore: KeystorePtr,
}

impl<FL, FS> StorageHubClientRpc<FL, FS>
where
    FL: FileStorage<StorageProofsMerkleTrieLayout> + Send + Sync,
    FS: ForestStorage<StorageProofsMerkleTrieLayout> + Send + Sync,
{
    pub fn new(storage_hub_client_rpc_config: StorageHubClientRpcConfig<FL, FS>) -> Self {
        Self {
            file_storage: storage_hub_client_rpc_config.file_storage,
            forest_storage: storage_hub_client_rpc_config.forest_storage,
            keystore: storage_hub_client_rpc_config.keystore,
        }
    }
}

/// Interface generated by the `rpc` macro from our `StorageHubClientApi` trait.
// TODO: Currently the UserSendsFile task will react to all runtime events triggered by
// file uploads, even if the file is not in its storage. So we need a way to inform the task
// to only react to its file.
#[async_trait]
impl<FL, FS> StorageHubClientApiServer for StorageHubClientRpc<FL, FS>
where
    FL: Send + Sync + FileStorage<StorageProofsMerkleTrieLayout>,
    FS: Send + Sync + ForestStorage<StorageProofsMerkleTrieLayout>,
{
    async fn load_file_in_storage(
        &self,
        file_path: String,
        location: String,
        owner: AccountId32,
        bucket_id: H256,
    ) -> RpcResult<FileMetadata> {
        // Open file in the local file system.
        let mut file = File::open(PathBuf::from(file_path.clone())).map_err(into_rpc_error)?;

        // Instantiate an "empty" [`FileDataTrie`] so we can write the file chunks into it.
        let mut file_data_trie = self.file_storage.write().await.new_file_data_trie();
        // A chunk id is simply an integer index.
        let mut chunk_id: u64 = 0;

        // Read file in chunks of [`FILE_CHUNK_SIZE`] into buffer then push buffer into a vector.
        // Loops until EOF or until some error that is NOT `ErrorKind::Interrupted` is found.
        // If `ErrorKind::Interrupted` is found, the operation is simply retried, as per
        // https://doc.rust-lang.org/std/io/trait.Read.html#errors-1
        loop {
            let mut chunk = Vec::with_capacity(FILE_CHUNK_SIZE as usize);
            let read_result = <File as Read>::by_ref(&mut file)
                .take(FILE_CHUNK_SIZE)
                .read_to_end(&mut chunk);
            match read_result {
                // Reached EOF, break loop.
                Ok(0) => {
                    debug!(target: LOG_TARGET, "Finished reading file");
                    break;
                }
                // Haven't reached EOF yet, continue loop.
                Ok(bytes_read) => {
                    debug!(target: LOG_TARGET, "Read {} bytes from file", bytes_read);

                    // Build the actual [`FileDataTrie`] by inserting each chunk into it.
                    file_data_trie
                        .write_chunk(&ChunkId::new(chunk_id), &chunk)
                        .map_err(into_rpc_error)?;
                    chunk_id += 1;
                }
                Err(e) => {
                    error!(target: LOG_TARGET, "Error when trying to read file: {:?}", e);
                    return Err(into_rpc_error(e));
                }
            }
        }

        // Generate the necessary metadata so we can insert file into the File Storage.
        let root = file_data_trie.get_root();
        let fs_metadata = file.metadata().map_err(into_rpc_error)?;

        if fs_metadata.len() == 0 {
            return Err(into_rpc_error(FileStorageError::FileIsEmpty));
        }

        // Build StorageHub's [`FileMetadata`]
        let file_metadata = FileMetadata {
            owner: <AccountId32 as AsRef<[u8]>>::as_ref(&owner).to_vec(),
            bucket_id: bucket_id.as_ref().to_vec(),
            file_size: fs_metadata.len(),
            fingerprint: root.as_ref().into(),
            location: location.clone().into(),
        };
        let file_key = file_metadata.file_key::<HashT<StorageProofsMerkleTrieLayout>>();

        // Acquire FileStorage write lock.
        let mut file_storage_write_lock = self.file_storage.write().await;

        // Finally store file in File Storage.
        file_storage_write_lock
            .insert_file_with_data(file_key, file_metadata.clone(), file_data_trie)
            .map_err(into_rpc_error)?;

        Ok(file_metadata)
    }

    async fn save_file_to_disk(
        &self,
        file_key: H256,
        file_path: String,
    ) -> RpcResult<SaveFileToDisk> {
        // Acquire FileStorage read lock.
        let read_file_storage = self.file_storage.read().await;

        // Retrieve file metadata from File Storage.
        let file_metadata = match read_file_storage.get_metadata(&file_key) {
            Ok(metadata) => metadata,
            Err(FileStorageError::FileDoesNotExist) => return Ok(SaveFileToDisk::FileNotFound),
            Err(e) => return Err(into_rpc_error(e)),
        };

        // Check if file is incomplete.
        let stored_chunks = read_file_storage
            .stored_chunks_count(&file_key)
            .map_err(into_rpc_error)?;
        let total_chunks = file_metadata.chunks_count();

        if stored_chunks < total_chunks {
            return Ok(SaveFileToDisk::IncompleteFile(IncompleteFileStatus {
                file_metadata,
                stored_chunks,
                total_chunks,
            }));
        }

        let file_path = PathBuf::from(file_path.clone());

        // Create parent directories if they don't exist.
        create_dir_all(&file_path.parent().unwrap())
            .await
            .map_err(into_rpc_error)?;

        // Open file in the local file system.
        let mut file = File::create(PathBuf::from(file_path.clone())).map_err(into_rpc_error)?;

        // Write file data to disk.
        for chunk_id in 0..total_chunks {
            let chunk_id = ChunkId::new(chunk_id);
            let chunk = read_file_storage
                .get_chunk(&file_key, &chunk_id)
                .map_err(into_rpc_error)?;
            file.write_all(&chunk).map_err(into_rpc_error)?;
        }

        Ok(SaveFileToDisk::Success(file_metadata))
    }

    async fn get_forest_root(&self) -> RpcResult<H256> {
        let forest_storage_read_lock = self.forest_storage.read().await;
        Ok(forest_storage_read_lock.root())
    }

    // If a seed is provided, we manually generate and persist it into the file system.
    // In the case a seed is not provided, we delegate generation and insertion to `sr25519_generate_new`, which
    // internally uses the block number as a seed.
    // See https://paritytech.github.io/polkadot-sdk/master/sc_keystore/struct.LocalKeystore.html#method.sr25519_generate_new
    async fn insert_bcsv_keys(&self, seed: Option<String>) -> RpcResult<String> {
        let seed = seed.as_deref();

        let new_pub_key = match seed {
            None => self
                .keystore
                .sr25519_generate_new(BCSV_KEY_TYPE, seed)
                .map_err(into_rpc_error)?,
            Some(seed) => {
                let new_pair = Sr25519Pair::from_string(seed, None).map_err(into_rpc_error)?;
                let new_pub_key = new_pair.public();
                self.keystore
                    .insert(BCSV_KEY_TYPE, seed, &new_pub_key)
                    .map_err(into_rpc_error)?;

                new_pub_key
            }
        };

        Ok(new_pub_key.to_string())
    }

    // Deletes all files with keys of type BCSV from the Keystore.
    async fn remove_bcsv_keys(&self, keystore_path: String) -> RpcResult<()> {
        let pub_keys = self.keystore.keys(BCSV_KEY_TYPE).map_err(into_rpc_error)?;
        let key_path = PathBuf::from(keystore_path);

        for pub_key in pub_keys {
            let mut key = key_path.clone();
            let key_name = key_file_name(&pub_key, BCSV_KEY_TYPE);
            key.push(key_name);

            // In case a key is not found we just ignore it
            // because there may be keys in memory that are not in the file system.
            let _ = fs::remove_file(key).await.map_err(|e| {
                error!(target: LOG_TARGET, "Failed to remove key: {:?}", e);
            });
        }

        Ok(())
    }
}

/// Get the file name for the given public key and key type.
fn key_file_name(public: &[u8], key_type: KeyTypeId) -> PathBuf {
    let mut buf = PathBuf::new();
    let key_type = array_bytes::bytes2hex("", &key_type.0);
    let key = array_bytes::bytes2hex("", public);
    buf.push(key_type + key.as_str());
    buf
}

/// Converts into the expected kind of error for `jsonrpsee`'s `RpcResult<_>`.
fn into_rpc_error(e: impl Debug) -> JsonRpseeError {
    JsonRpseeError::owned(
        INTERNAL_ERROR_CODE,
        INTERNAL_ERROR_MSG,
        Some(format!("{:?}", e)),
    )
}
